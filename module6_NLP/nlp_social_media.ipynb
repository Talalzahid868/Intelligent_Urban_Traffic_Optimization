{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Missing dependency: No module named 'transformers'\n",
                        "Install with: pip install transformers torch nltk\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'nltk' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInstall with: pip install transformers torch nltk\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Download NLTK data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mnltk\u001b[49m.download(\u001b[33m\"\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m\"\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m nltk.download(\u001b[33m\"\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m\"\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m nltk.download(\u001b[33m\"\u001b[39m\u001b[33maveraged_perceptron_tagger\u001b[39m\u001b[33m\"\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'nltk' is not defined"
                    ]
                }
            ],
            "source": [
                "# Core libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# NLP libraries\n",
                "try:\n",
                "    import torch\n",
                "    from transformers import (\n",
                "        AutoTokenizer,\n",
                "        AutoModelForSequenceClassification,\n",
                "        AutoModelForTokenClassification,\n",
                "        pipeline,\n",
                "    )\n",
                "    import nltk\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    from nltk.corpus import stopwords\n",
                "except ImportError as e:\n",
                "    print(f\"Missing dependency: {e}\")\n",
                "    print(\"Install with: pip install transformers torch nltk\")\n",
                "\n",
                "# Download NLTK data\n",
                "nltk.download(\"punkt\", quiet=True)\n",
                "nltk.download(\"stopwords\", quiet=True)\n",
                "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
                "\n",
                "print(\"âœ“ Dependencies loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Sample Traffic-Related Social Media Data\n",
                "\n",
                "Creating synthetic social media posts for demonstration. In production, this would be collected from Twitter/X API, local news feeds, or traffic reporting platforms.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample social media posts about traffic\n",
                "sample_posts = [\n",
                "    # Accidents and incidents\n",
                "    \"Major accident on Highway 101 near downtown exit. Traffic completely stopped! ðŸš—ðŸ’¥\",\n",
                "    \"Car crash on Main Street blocking two lanes. Expect delays of 30+ minutes.\",\n",
                "    \"Overturned truck on I-95 southbound. Emergency services on scene.\",\n",
                "    # Construction and road work\n",
                "    \"Road construction on Oak Avenue starting Monday. Lane closures expected for 2 weeks.\",\n",
                "    \"Bridge repair work causing single lane traffic on River Road.\",\n",
                "    \"New traffic lights being installed at 5th and Broadway intersection.\",\n",
                "    # Events impacting traffic\n",
                "    \"Big concert at City Arena tonight! Traffic around downtown will be crazy.\",\n",
                "    \"Football game at the stadium - leave early to avoid the rush!\",\n",
                "    \"Marathon running through city center tomorrow. Multiple road closures.\",\n",
                "    # General traffic conditions\n",
                "    \"Traffic flowing smoothly on the freeway this morning. Great commute!\",\n",
                "    \"Ugh, stuck in gridlock again on the 405. This city needs better transit.\",\n",
                "    \"Loving the new bike lanes - traffic seems lighter since they were added.\",\n",
                "    # Weather-related\n",
                "    \"Heavy rain causing slow traffic everywhere. Drive carefully!\",\n",
                "    \"Fog advisory - visibility poor on coastal highways.\",\n",
                "    \"Ice on roads this morning. Several fender benders reported.\",\n",
                "]\n",
                "\n",
                "# Create DataFrame\n",
                "df_posts = pd.DataFrame(\n",
                "    {\n",
                "        \"post_id\": range(1, len(sample_posts) + 1),\n",
                "        \"text\": sample_posts,\n",
                "        \"timestamp\": pd.date_range(\n",
                "            start=\"2024-01-15 06:00\", periods=len(sample_posts), freq=\"2H\"\n",
                "        ),\n",
                "    }\n",
                ")\n",
                "\n",
                "print(f\"Sample dataset: {len(df_posts)} posts\")\n",
                "df_posts.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Text Preprocessing Pipeline\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrafficTextPreprocessor:\n",
                "    \"\"\"Text preprocessing for traffic-related social media content.\"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        self.stop_words = set(stopwords.words(\"english\"))\n",
                "        # Traffic-specific terms to keep even if they're stopwords\n",
                "        self.keep_words = {\"no\", \"not\", \"up\", \"down\", \"out\", \"off\", \"on\"}\n",
                "\n",
                "    def clean_text(self, text: str) -> str:\n",
                "        \"\"\"Clean and normalize text.\"\"\"\n",
                "        # Lowercase\n",
                "        text = text.lower()\n",
                "\n",
                "        # Remove URLs\n",
                "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
                "\n",
                "        # Remove mentions and hashtags but keep the content\n",
                "        text = re.sub(r\"@\\w+\", \"\", text)\n",
                "        text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
                "\n",
                "        # Remove emojis but keep punctuation for sentiment\n",
                "        text = re.sub(r\"[^\\w\\s.,!?-]\", \"\", text)\n",
                "\n",
                "        # Normalize whitespace\n",
                "        text = \" \".join(text.split())\n",
                "\n",
                "        return text.strip()\n",
                "\n",
                "    def tokenize(self, text: str) -> list:\n",
                "        \"\"\"Tokenize text.\"\"\"\n",
                "        return word_tokenize(text)\n",
                "\n",
                "    def preprocess(self, text: str) -> str:\n",
                "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
                "        cleaned = self.clean_text(text)\n",
                "        return cleaned\n",
                "\n",
                "\n",
                "# Initialize preprocessor\n",
                "preprocessor = TrafficTextPreprocessor()\n",
                "\n",
                "# Apply preprocessing\n",
                "df_posts[\"cleaned_text\"] = df_posts[\"text\"].apply(preprocessor.preprocess)\n",
                "\n",
                "# Show example\n",
                "print(\"Original:\", df_posts[\"text\"].iloc[0])\n",
                "print(\"Cleaned: \", df_posts[\"cleaned_text\"].iloc[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Traffic Event Classification\n",
                "\n",
                "Using a pre-trained transformer model for classifying traffic-related events.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define traffic event categories\n",
                "TRAFFIC_EVENTS = {\n",
                "    \"accident\": [\n",
                "        \"accident\",\n",
                "        \"crash\",\n",
                "        \"collision\",\n",
                "        \"overturned\",\n",
                "        \"wreck\",\n",
                "        \"fender bender\",\n",
                "    ],\n",
                "    \"construction\": [\n",
                "        \"construction\",\n",
                "        \"roadwork\",\n",
                "        \"repair\",\n",
                "        \"maintenance\",\n",
                "        \"closure\",\n",
                "        \"lane closure\",\n",
                "    ],\n",
                "    \"event\": [\"concert\", \"game\", \"marathon\", \"parade\", \"festival\", \"stadium\", \"arena\"],\n",
                "    \"weather\": [\"rain\", \"snow\", \"ice\", \"fog\", \"storm\", \"flood\", \"visibility\"],\n",
                "    \"congestion\": [\"traffic\", \"gridlock\", \"jam\", \"slow\", \"backed up\", \"delay\", \"stuck\"],\n",
                "}\n",
                "\n",
                "\n",
                "def classify_traffic_event(text: str) -> dict:\n",
                "    \"\"\"Rule-based traffic event classification.\"\"\"\n",
                "    text_lower = text.lower()\n",
                "    detected_events = []\n",
                "\n",
                "    for event_type, keywords in TRAFFIC_EVENTS.items():\n",
                "        for keyword in keywords:\n",
                "            if keyword in text_lower:\n",
                "                detected_events.append(event_type)\n",
                "                break\n",
                "\n",
                "    # Determine primary event and severity\n",
                "    severity = \"low\"\n",
                "    if \"accident\" in detected_events:\n",
                "        severity = \"high\"\n",
                "    elif \"construction\" in detected_events or \"event\" in detected_events:\n",
                "        severity = \"medium\"\n",
                "    elif \"weather\" in detected_events:\n",
                "        severity = \"medium\"\n",
                "\n",
                "    return {\n",
                "        \"events\": list(set(detected_events)) if detected_events else [\"general\"],\n",
                "        \"severity\": severity,\n",
                "        \"traffic_impact\": len(detected_events) > 0,\n",
                "    }\n",
                "\n",
                "\n",
                "# Apply classification\n",
                "df_posts[\"event_classification\"] = df_posts[\"cleaned_text\"].apply(\n",
                "    classify_traffic_event\n",
                ")\n",
                "\n",
                "# Extract for easier analysis\n",
                "df_posts[\"events\"] = df_posts[\"event_classification\"].apply(lambda x: x[\"events\"])\n",
                "df_posts[\"severity\"] = df_posts[\"event_classification\"].apply(lambda x: x[\"severity\"])\n",
                "df_posts[\"has_traffic_impact\"] = df_posts[\"event_classification\"].apply(\n",
                "    lambda x: x[\"traffic_impact\"]\n",
                ")\n",
                "\n",
                "print(\"Event classification complete!\")\n",
                "df_posts[[\"text\", \"events\", \"severity\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sentiment Analysis\n",
                "\n",
                "Using HuggingFace's pre-trained sentiment model to analyze traffic-related sentiment.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize sentiment analysis pipeline\n",
                "print(\"Loading sentiment analysis model...\")\n",
                "sentiment_analyzer = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "    device=-1,  # CPU, use 0 for GPU\n",
                ")\n",
                "print(\"âœ“ Model loaded\")\n",
                "\n",
                "\n",
                "def analyze_sentiment(text: str) -> dict:\n",
                "    \"\"\"Analyze sentiment of traffic-related text.\"\"\"\n",
                "    try:\n",
                "        result = sentiment_analyzer(text[:512])[0]  # Truncate to max length\n",
                "        return {\"label\": result[\"label\"], \"score\": round(result[\"score\"], 4)}\n",
                "    except Exception as e:\n",
                "        return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
                "\n",
                "\n",
                "# Apply sentiment analysis\n",
                "df_posts[\"sentiment\"] = df_posts[\"cleaned_text\"].apply(analyze_sentiment)\n",
                "df_posts[\"sentiment_label\"] = df_posts[\"sentiment\"].apply(lambda x: x[\"label\"])\n",
                "df_posts[\"sentiment_score\"] = df_posts[\"sentiment\"].apply(lambda x: x[\"score\"])\n",
                "\n",
                "print(\"\\nSentiment Analysis Results:\")\n",
                "df_posts[[\"text\", \"sentiment_label\", \"sentiment_score\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Named Entity Recognition (Location Extraction)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize NER pipeline\n",
                "print(\"Loading NER model...\")\n",
                "ner_analyzer = pipeline(\n",
                "    \"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=-1\n",
                ")\n",
                "print(\"âœ“ NER model loaded\")\n",
                "\n",
                "\n",
                "def extract_locations(text: str) -> list:\n",
                "    \"\"\"Extract location entities from text.\"\"\"\n",
                "    try:\n",
                "        entities = ner_analyzer(text)\n",
                "        locations = [ent[\"word\"] for ent in entities if ent[\"entity_group\"] == \"LOC\"]\n",
                "        return locations\n",
                "    except Exception as e:\n",
                "        return []\n",
                "\n",
                "\n",
                "# Apply NER\n",
                "df_posts[\"locations\"] = df_posts[\"text\"].apply(extract_locations)\n",
                "\n",
                "print(\"\\nLocation Extraction Results:\")\n",
                "df_posts[[\"text\", \"locations\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Traffic Impact Scoring\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_traffic_impact_score(row) -> float:\n",
                "    \"\"\"Calculate overall traffic impact score (0-100).\"\"\"\n",
                "    score = 0\n",
                "\n",
                "    # Event severity contribution\n",
                "    severity_scores = {\"high\": 40, \"medium\": 25, \"low\": 10}\n",
                "    score += severity_scores.get(row[\"severity\"], 0)\n",
                "\n",
                "    # Negative sentiment indicates frustration (higher impact)\n",
                "    if row[\"sentiment_label\"] == \"NEGATIVE\":\n",
                "        score += 20 * row[\"sentiment_score\"]\n",
                "\n",
                "    # More event types = higher complexity\n",
                "    score += len(row[\"events\"]) * 5\n",
                "\n",
                "    # Location specificity\n",
                "    if len(row[\"locations\"]) > 0:\n",
                "        score += 10\n",
                "\n",
                "    return min(100, round(score, 2))\n",
                "\n",
                "\n",
                "df_posts[\"impact_score\"] = df_posts.apply(calculate_traffic_impact_score, axis=1)\n",
                "\n",
                "# Sort by impact\n",
                "df_sorted = df_posts.sort_values(\"impact_score\", ascending=False)\n",
                "\n",
                "print(\"\\nðŸ“Š Top Traffic Impact Posts:\")\n",
                "df_sorted[[\"text\", \"events\", \"severity\", \"impact_score\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. Event Type Distribution\n",
                "all_events = [event for events in df_posts[\"events\"] for event in events]\n",
                "event_counts = pd.Series(all_events).value_counts()\n",
                "axes[0, 0].bar(event_counts.index, event_counts.values, color=\"steelblue\")\n",
                "axes[0, 0].set_title(\"Traffic Event Types Distribution\", fontsize=12, fontweight=\"bold\")\n",
                "axes[0, 0].set_xlabel(\"Event Type\")\n",
                "axes[0, 0].set_ylabel(\"Count\")\n",
                "axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
                "\n",
                "# 2. Severity Distribution\n",
                "severity_counts = df_posts[\"severity\"].value_counts()\n",
                "colors = {\"high\": \"#e74c3c\", \"medium\": \"#f39c12\", \"low\": \"#2ecc71\"}\n",
                "axes[0, 1].pie(\n",
                "    severity_counts.values,\n",
                "    labels=severity_counts.index,\n",
                "    autopct=\"%1.1f%%\",\n",
                "    colors=[colors[s] for s in severity_counts.index],\n",
                ")\n",
                "axes[0, 1].set_title(\"Event Severity Distribution\", fontsize=12, fontweight=\"bold\")\n",
                "\n",
                "# 3. Sentiment Distribution\n",
                "sentiment_counts = df_posts[\"sentiment_label\"].value_counts()\n",
                "axes[1, 0].bar(\n",
                "    sentiment_counts.index, sentiment_counts.values, color=[\"#27ae60\", \"#e74c3c\"]\n",
                ")\n",
                "axes[1, 0].set_title(\"Sentiment Analysis Results\", fontsize=12, fontweight=\"bold\")\n",
                "axes[1, 0].set_xlabel(\"Sentiment\")\n",
                "axes[1, 0].set_ylabel(\"Count\")\n",
                "\n",
                "# 4. Impact Score Distribution\n",
                "axes[1, 1].hist(\n",
                "    df_posts[\"impact_score\"], bins=10, color=\"purple\", edgecolor=\"white\", alpha=0.7\n",
                ")\n",
                "axes[1, 1].axvline(\n",
                "    df_posts[\"impact_score\"].mean(),\n",
                "    color=\"red\",\n",
                "    linestyle=\"--\",\n",
                "    label=f'Mean: {df_posts[\"impact_score\"].mean():.1f}',\n",
                ")\n",
                "axes[1, 1].set_title(\n",
                "    \"Traffic Impact Score Distribution\", fontsize=12, fontweight=\"bold\"\n",
                ")\n",
                "axes[1, 1].set_xlabel(\"Impact Score\")\n",
                "axes[1, 1].set_ylabel(\"Frequency\")\n",
                "axes[1, 1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"nlp_analysis_results.png\", dpi=150, bbox_inches=\"tight\")\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nâœ“ Visualization saved to nlp_analysis_results.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. NLP Wrapper Class for Integration\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrafficNLPAnalyzer:\n",
                "    \"\"\"\n",
                "    Complete NLP analyzer for traffic-related social media content.\n",
                "    This class is used by the integration layer wrapper.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, device: int = -1):\n",
                "        \"\"\"\n",
                "        Initialize the NLP analyzer.\n",
                "\n",
                "        Args:\n",
                "            device: -1 for CPU, 0+ for GPU\n",
                "        \"\"\"\n",
                "        self.device = device\n",
                "        self.preprocessor = TrafficTextPreprocessor()\n",
                "        self._sentiment_model = None\n",
                "        self._ner_model = None\n",
                "\n",
                "    def _load_models(self):\n",
                "        \"\"\"Lazy load NLP models.\"\"\"\n",
                "        if self._sentiment_model is None:\n",
                "            self._sentiment_model = pipeline(\n",
                "                \"sentiment-analysis\",\n",
                "                model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "                device=self.device,\n",
                "            )\n",
                "        if self._ner_model is None:\n",
                "            self._ner_model = pipeline(\n",
                "                \"ner\",\n",
                "                model=\"dslim/bert-base-NER\",\n",
                "                aggregation_strategy=\"simple\",\n",
                "                device=self.device,\n",
                "            )\n",
                "\n",
                "    def analyze(self, text: str) -> dict:\n",
                "        \"\"\"\n",
                "        Full analysis of a social media post.\n",
                "\n",
                "        Returns:\n",
                "            Dictionary with event detection, sentiment, locations, and impact score.\n",
                "        \"\"\"\n",
                "        self._load_models()\n",
                "\n",
                "        # Preprocess\n",
                "        cleaned_text = self.preprocessor.preprocess(text)\n",
                "\n",
                "        # Event classification\n",
                "        event_info = classify_traffic_event(cleaned_text)\n",
                "\n",
                "        # Sentiment\n",
                "        sentiment = analyze_sentiment(cleaned_text)\n",
                "\n",
                "        # Locations\n",
                "        locations = extract_locations(text)\n",
                "\n",
                "        # Calculate impact score\n",
                "        impact_score = self._calculate_impact(event_info, sentiment, locations)\n",
                "\n",
                "        return {\n",
                "            \"original_text\": text,\n",
                "            \"cleaned_text\": cleaned_text,\n",
                "            \"events\": event_info[\"events\"],\n",
                "            \"severity\": event_info[\"severity\"],\n",
                "            \"sentiment\": sentiment,\n",
                "            \"locations\": locations,\n",
                "            \"impact_score\": impact_score,\n",
                "            \"has_traffic_impact\": event_info[\"traffic_impact\"],\n",
                "        }\n",
                "\n",
                "    def _calculate_impact(self, event_info, sentiment, locations) -> float:\n",
                "        \"\"\"Calculate traffic impact score.\"\"\"\n",
                "        score = 0\n",
                "        severity_scores = {\"high\": 40, \"medium\": 25, \"low\": 10}\n",
                "        score += severity_scores.get(event_info[\"severity\"], 0)\n",
                "\n",
                "        if sentiment[\"label\"] == \"NEGATIVE\":\n",
                "            score += 20 * sentiment[\"score\"]\n",
                "\n",
                "        score += len(event_info[\"events\"]) * 5\n",
                "\n",
                "        if len(locations) > 0:\n",
                "            score += 10\n",
                "\n",
                "        return min(100, round(score, 2))\n",
                "\n",
                "    def analyze_batch(self, texts: list) -> list:\n",
                "        \"\"\"Analyze multiple posts.\"\"\"\n",
                "        return [self.analyze(text) for text in texts]\n",
                "\n",
                "\n",
                "# Test the analyzer\n",
                "analyzer = TrafficNLPAnalyzer()\n",
                "test_result = analyzer.analyze(\n",
                "    \"Major accident on Highway 101! Traffic is completely stopped.\"\n",
                ")\n",
                "print(\"\\nTest Analysis Result:\")\n",
                "print(json.dumps(test_result, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export Results and Summary\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export analyzed data\n",
                "output_df = df_posts[\n",
                "    [\n",
                "        \"post_id\",\n",
                "        \"text\",\n",
                "        \"events\",\n",
                "        \"severity\",\n",
                "        \"sentiment_label\",\n",
                "        \"sentiment_score\",\n",
                "        \"locations\",\n",
                "        \"impact_score\",\n",
                "    ]\n",
                "]\n",
                "output_df.to_csv(\"nlp_analysis_results.csv\", index=False)\n",
                "print(\"âœ“ Results exported to nlp_analysis_results.csv\")\n",
                "\n",
                "# Summary statistics\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ðŸ“Š NLP ANALYSIS SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Total posts analyzed: {len(df_posts)}\")\n",
                "print(f\"Posts with traffic impact: {df_posts['has_traffic_impact'].sum()}\")\n",
                "print(f\"High severity events: {(df_posts['severity'] == 'high').sum()}\")\n",
                "print(f\"Average impact score: {df_posts['impact_score'].mean():.2f}\")\n",
                "print(f\"Negative sentiment posts: {(df_posts['sentiment_label'] == 'NEGATIVE').sum()}\")\n",
                "print(f\"Posts with location data: {(df_posts['locations'].apply(len) > 0).sum()}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Integration with Traffic Optimization Pipeline\n",
                "\n",
                "The NLP module integrates with the main traffic optimization system to provide:\n",
                "\n",
                "1. **Early Warning System**: Detect accidents and events before sensor data shows impact\n",
                "2. **Context Enhancement**: Add social media context to anomaly detection\n",
                "3. **Demand Forecasting**: Predict traffic surges from event announcements\n",
                "4. **Signal Optimization**: Adjust RL recommendations based on real-time social signals\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nâœ… Module 6: NLP Social Media Analysis - Complete!\")\n",
                "print(\"\\nThis module can be integrated via:\")\n",
                "print(\"  from integration.wrappers.nlp_wrapper import NLPWrapper\")\n",
                "print(\"  nlp = NLPWrapper()\")\n",
                "print(\"  result = nlp.analyze_text('Traffic alert text...')\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
